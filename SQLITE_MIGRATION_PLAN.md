# SQLite Migration Plan

## Goal
Replace JSON catalog with SQLite database in the backend.

## Why
- Local `catalog.json` and S3 `catalog.json` are out of sync
- S3 has `"Standard (Concert)"` while local has `"Standard Key"`
- SQLite is already generated by `build_catalog.py` and has correct data
- SQLite allows more efficient queries (pagination, filtering at DB level)

## Current State

**Files involved:**
- `catalog.json` (5.7MB) - current source of truth, loaded from S3
- `catalog.db` (1.3MB) - SQLite, generated but unused
- `app.py` - Flask backend, reads from JSON

**DB Schema (already exists):**
```sql
songs (id, title, core_files, created_at)
variations (id, song_id, filename, display_name, key, instrument, variation_type, voice_range, pdf_path)
metadata (key, value)
```

## Migration Steps

### Step 1: Create `db.py` helper module ✅ (done)
- `init_db()` - initialize connection
- `get_song_by_title()` - replaces `catalog_data['songs'][title]`
- `search_songs()` - replaces iteration with filtering
- `get_song_default_key()` - for the new cached keys endpoint
- `get_core_files()` - for PDF generation

### Step 2: Update `app.py` endpoints

**Changes needed:**

| Endpoint | Current | New |
|----------|---------|-----|
| `/health` | `catalog_data['metadata']` | `db.get_metadata()` |
| `/api/v2/songs` | iterate `catalog_data['songs'].items()` | `db.search_songs()` |
| `/api/v2/songs/<title>` | `catalog_data['songs'][title]` | `db.get_song_by_title()` |
| `/api/v2/songs/<title>/cached` | `catalog_data['songs'][title]` | `db.get_song_default_key()` |
| `/api/v2/generate` | `catalog_data['songs'][title]` | `db.get_core_files()` |

**Remove:**
- `load_catalog()` function
- Global `catalog_data` and `catalog_etag` variables
- S3 catalog download logic

**Add:**
- Import `db` module
- Call `db.init_db()` at startup
- ETag can be based on DB file modification time

### Step 3: Update deployment

**Dockerfile.prod:**
- Copy `catalog.db` into the image (instead of fetching JSON from S3)
- Or: Download `catalog.db` from S3 at startup

**Catalog updates:**
- Run `build_catalog.py` to regenerate `catalog.db`
- Rebuild and deploy Docker image
- (Simpler than syncing JSON to S3)

### Step 4: Cleanup

- Remove unused JSON endpoints (if any)
- Update `build_catalog.py` if needed (it already outputs SQLite)
- Update CLAUDE.md documentation

## Endpoints to Update

1. **`GET /health`** - show song/variation counts from DB
2. **`GET /api/v2/songs`** - paginated song list with filtering
3. **`GET /api/v2/songs/<title>`** - song detail with variations
4. **`GET /api/v2/songs/<title>/cached`** - default key + cached keys
5. **`POST /api/v2/generate`** - get core_files for generation

## Risk Assessment

**Low risk:**
- DB schema already matches JSON structure
- Can test locally before deploying
- Can keep JSON as fallback initially

**Testing:**
- Verify all 5 endpoints work with SQLite
- Check pagination works correctly
- Verify filtering (instrument, search query)
- Test PDF generation still works

## Auto-Refresh When Eric Updates Charts

**Context:** Eric's lilypond files are in a git repo. When he commits changes, we want the catalog to update automatically.

### Option A: GitHub Actions (recommended)
```
Eric pushes to lilypond repo
    → GitHub Action triggers
    → Runs build_catalog.py
    → Uploads catalog.db to S3
    → Calls Fly.io deploy (or just restarts app)
    → App downloads fresh catalog.db on startup
```

**Pros:** Fully automated, no manual steps
**Cons:** Need access to Eric's repo to add the Action

### Option B: Webhook + On-Demand Rebuild
```
Eric pushes to lilypond repo
    → GitHub webhook calls our backend endpoint
    → Backend pulls latest files, runs build_catalog.py
    → Reloads catalog in memory
```

**Pros:** No redeploy needed
**Cons:** Backend needs git access to lilypond repo, more complex

### Option C: Scheduled Rebuild
```
Cron job (daily/hourly)
    → Check if lilypond repo has new commits
    → If yes, rebuild catalog and redeploy
```

**Pros:** Simple, no webhook setup
**Cons:** Not instant, delay between commit and update

### Option D: Manual (current)
```
Eric notifies you
    → You run build_catalog.py locally
    → fly deploy
```

**Decision:** Option A - GitHub Action in Eric's repo.

## GitHub Action Setup

Add `.github/workflows/update-catalog.yml` to Eric's lilypond repo:

```yaml
name: Update Jazz Picker Catalog

on:
  push:
    branches: [main]
    paths:
      - 'Wrappers/**'
      - 'Core/**'

jobs:
  update-catalog:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Checkout jazz-picker
        uses: actions/checkout@v4
        with:
          repository: <your-org>/jazz-picker
          path: jazz-picker
          token: ${{ secrets.JAZZ_PICKER_TOKEN }}

      - name: Build catalog
        run: |
          cd jazz-picker
          python build_catalog.py

      - name: Upload to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          aws s3 cp jazz-picker/catalog.db s3://jazz-picker-pdfs/catalog.db

      - name: Restart Fly app
        env:
          FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}
        run: |
          curl -X POST "https://api.fly.io/v1/apps/jazz-picker/machines" \
            -H "Authorization: Bearer $FLY_API_TOKEN" \
            -H "Content-Type: application/json"
          # Or simpler: flyctl apps restart jazz-picker
```

**Secrets needed in Eric's repo:**
- `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY` - for S3 upload
- `FLY_API_TOKEN` - for restart (get via `fly tokens create deploy`)
- `JAZZ_PICKER_TOKEN` - GitHub PAT to checkout jazz-picker repo (if private)

## Where to Store catalog.db

1. Backend downloads `catalog.db` from S3 on startup
2. GitHub Action uploads new `catalog.db` to S3 after rebuild
3. Fly app restarts to pick up new catalog

---

# Future Work: Domain + Auth

## Custom Domain (GoDaddy)

**Frontend is on Vercel** (frontend-phi-khaki-43.vercel.app)

Steps:
1. In GoDaddy DNS, add CNAME record pointing to Vercel
2. In Vercel dashboard, add custom domain
3. Vercel handles SSL automatically

## Auth Implementation (phased)

### Phase 1: Simple Password Protection
- Add password prompt to frontend (stored in localStorage after entry)
- Backend already has `REQUIRE_AUTH` + basic auth
- Quick to implement, good enough for limited access

### Phase 2: User Accounts
- Options: Clerk, Auth0, Supabase Auth, or custom
- Store user preferences (default key, instrument)
- Enable setlists (per-user)

### Phase 3: Roles
- Admin: can trigger cache invalidation, view stats
- User: normal access

**Recommendation:** Start with Phase 1 (password gate), then add proper auth when setlists feature is ready.
